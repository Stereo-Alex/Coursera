\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    
    \usepackage{iftex}
    \ifPDFTeX
    	\usepackage[T1]{fontenc}
    	\usepackage{mathpazo}
    \else
    	\usepackage{fontspec}
    \fi

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}
    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range
    \makeatletter % fix for grffile with XeLaTeX
    \def\Gread@@xetex#1{%
      \IfFileExists{"\Gin@base".bb}%
      {\Gread@eps{\Gin@base.bb}}%
      {\Gread@@xetex@aux#1}%
    }
    \makeatother

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{Operations\_on\_word\_vectors\_v2a}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb. 
    \makeatletter
        \newbox\Wrappedcontinuationbox 
        \newbox\Wrappedvisiblespacebox 
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}} 
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}} 
        \newcommand*\Wrappedcontinuationindent {3ex } 
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox} 
        % Take advantage of the already applied Pygments mark-up to insert 
        % potential linebreaks for TeX processing. 
        %        {, <, #, %, $, ' and ": go to next line. 
        %        _, }, ^, &, >, - and ~: stay at end of broken line. 
        % Use of \textquotesingle for straight quote. 
        \newcommand*\Wrappedbreaksatspecials {% 
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}% 
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}% 
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}% 
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}% 
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}% 
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}% 
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}% 
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}% 
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}% 
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}% 
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}% 
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}% 
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}% 
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}% 
        } 
        % Some characters . , ; ? ! / are not pygmentized. 
        % This macro makes them "active" and they will insert potential linebreaks 
        \newcommand*\Wrappedbreaksatpunct {% 
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}% 
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}% 
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}% 
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}% 
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}% 
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}% 
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}% 
            \catcode`\.\active
            \catcode`\,\active 
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active 
            \lccode`\~`\~ 	
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%
        
        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active 	
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}
    
    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        \ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \hypertarget{operations-on-word-vectors}{%
\section{Operations on Word Vectors}\label{operations-on-word-vectors}}

Welcome to your first assignment of Week 2, Course 5 of the Deep
Learning Specialization!

Because word embeddings are very computationally expensive to train,
most ML practitioners will load a pre-trained set of embeddings. In this
notebook you'll try your hand at loading, measuring similarity between,
and modifying pre-trained embeddings.

\textbf{After this assignment you'll be able to}:

\begin{itemize}
\tightlist
\item
  Explain how word embeddings capture relationships between words
\item
  Load pre-trained word vectors
\item
  Measure similarity between word vectors using cosine similarity
\item
  Use word embeddings to solve word analogy problems such as Man is to
  Woman as King is to \_\_\_\_\_\_.
\end{itemize}

At the end of this notebook you'll have a chance to try an optional
exercise, where you'll modify word embeddings to reduce their gender
bias. Reducing bias is an important consideration in ML, so you're
encouraged to take this challenge!

    \hypertarget{table-of-contents}{%
\subsection{Table of Contents}\label{table-of-contents}}

\begin{itemize}
\tightlist
\item
  Section \ref{0}
\item
  Section \ref{1}
\item
  Section \ref{2}
\item
  Section \ref{3}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{ex-1}
  \end{itemize}
\item
  Section \ref{4}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{ex-2}
  \end{itemize}
\item
  Section \ref{5}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{5-1}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex-3}
    \end{itemize}
  \item
    Section \ref{5-2}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex-4}
    \end{itemize}
  \end{itemize}
\item
  Section \ref{6}
\end{itemize}

    \#\# Packages

Let's get started! Run the following cell to load the packages you'll
need.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{from} \PY{n+nn}{w2v\PYZus{}utils} \PY{k+kn}{import} \PY{o}{*}
\end{Verbatim}
\end{tcolorbox}

    \#\# 1 - Load the Word Vectors

For this assignment, you'll use 50-dimensional GloVe vectors to
represent words. Run the following cell to load the
\texttt{word\_to\_vec\_map}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{words}\PY{p}{,} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map} \PY{o}{=} \PY{n}{read\PYZus{}glove\PYZus{}vecs}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{data/glove.6B.50d.txt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    You've loaded: - \texttt{words}: set of words in the vocabulary. -
\texttt{word\_to\_vec\_map}: dictionary mapping words to their GloVe
vector representation.

\#\# 2 - Embedding Vectors Versus One-Hot Vectors Recall from the lesson
videos that one-hot vectors don't do a good job of capturing the level
of similarity between words. This is because every one-hot vector has
the same Euclidean distance from any other one-hot vector.

Embedding vectors, such as GloVe vectors, provide much more useful
information about the meaning of individual words.\\
Now, see how you can use GloVe vectors to measure the similarity between
two words!

    \#\# 3 - Cosine Similarity

To measure the similarity between two words, you need a way to measure
the degree of similarity between two embedding vectors for the two
words. Given two vectors \(u\) and \(v\), cosine similarity is defined
as follows:

\[\text{CosineSimilarity(u, v)} = \frac {u \cdot v} {||u||_2 ||v||_2} = cos(\theta) \tag{1}\]

\begin{itemize}
\tightlist
\item
  \(u \cdot v\) is the dot product (or inner product) of two vectors
\item
  \(||u||_2\) is the norm (or length) of the vector \(u\)
\item
  \(\theta\) is the angle between \(u\) and \(v\).
\item
  The cosine similarity depends on the angle between \(u\) and \(v\).

  \begin{itemize}
  \tightlist
  \item
    If \(u\) and \(v\) are very similar, their cosine similarity will be
    close to 1.
  \item
    If they are dissimilar, the cosine similarity will take a smaller
    value.
  \end{itemize}
\end{itemize}

Figure 1: The cosine of the angle between two vectors is a measure of
their similarity.

\#\#\# Exercise 1 - cosine\_similarity

Implement the function \texttt{cosine\_similarity()} to evaluate the
similarity between word vectors.

\textbf{Reminder}: The norm of \(u\) is defined as \$
\textbar\textbar u\textbar\textbar\_2 = \sqrt{\sum_{i=1}^{n} u_i^2}\$

\hypertarget{additional-hints}{%
\paragraph{Additional Hints}\label{additional-hints}}

\begin{itemize}
\tightlist
\item
  You may find
  \href{https://numpy.org/doc/stable/reference/generated/numpy.dot.html}{np.dot},
  \href{https://numpy.org/doc/stable/reference/generated/numpy.sum.html}{np.sum},
  or
  \href{https://numpy.org/doc/stable/reference/generated/numpy.sqrt.html}{np.sqrt}
  useful depending upon the implementation that you choose.
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} UNQ\PYZus{}C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)}
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: cosine\PYZus{}similarity}

\PY{k}{def} \PY{n+nf}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{u}\PY{p}{,} \PY{n}{v}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Cosine similarity reflects the degree of similarity between u and v}
\PY{l+s+sd}{        }
\PY{l+s+sd}{    Arguments:}
\PY{l+s+sd}{        u \PYZhy{}\PYZhy{} a word vector of shape (n,)          }
\PY{l+s+sd}{        v \PYZhy{}\PYZhy{} a word vector of shape (n,)}

\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{        cosine\PYZus{}similarity \PYZhy{}\PYZhy{} the cosine similarity between u and v defined by the formula above.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{c+c1}{\PYZsh{} Special case. Consider the case u = [0, 0], v=[0, 0]}
    \PY{k}{if} \PY{n}{np}\PY{o}{.}\PY{n}{all}\PY{p}{(}\PY{n}{u} \PY{o}{==} \PY{n}{v}\PY{p}{)}\PY{p}{:}
        \PY{k}{return} \PY{l+m+mi}{1}
    
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
    \PY{c+c1}{\PYZsh{} Compute the dot product between u and v (≈1 line)}
    \PY{n}{dot} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{u}\PY{p}{,}\PY{n}{v}\PY{p}{)} 
    \PY{c+c1}{\PYZsh{} Compute the L2 norm of u (≈1 line)}
    \PY{n}{norm\PYZus{}u} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{u} \PY{o}{*} \PY{n}{u}\PY{p}{)}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Compute the L2 norm of v (≈1 line)}
    \PY{n}{norm\PYZus{}v} \PY{o}{=}  \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{v} \PY{o}{*} \PY{n}{v}\PY{p}{)}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Avoid division by 0}
    \PY{k}{if} \PY{n}{np}\PY{o}{.}\PY{n}{isclose}\PY{p}{(}\PY{n}{norm\PYZus{}u} \PY{o}{*} \PY{n}{norm\PYZus{}v}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{atol}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}32}\PY{p}{)}\PY{p}{:}
        \PY{k}{return} \PY{l+m+mi}{0}
    
    \PY{c+c1}{\PYZsh{} Compute the cosine similarity defined by formula (1) (≈1 line)}
    \PY{n}{cosine\PYZus{}similarity} \PY{o}{=} \PY{n}{dot} \PY{o}{/} \PY{p}{(}\PY{n}{norm\PYZus{}u} \PY{o}{*} \PY{n}{norm\PYZus{}v}\PY{p}{)}
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
    
    \PY{k}{return} \PY{n}{cosine\PYZus{}similarity}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} START SKIP FOR GRADING}
\PY{n}{father} \PY{o}{=} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{father}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\PY{n}{mother} \PY{o}{=} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mother}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\PY{n}{ball} \PY{o}{=} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ball}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\PY{n}{crocodile} \PY{o}{=} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{crocodile}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\PY{n}{france} \PY{o}{=} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{france}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\PY{n}{italy} \PY{o}{=} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{italy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\PY{n}{paris} \PY{o}{=} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{paris}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\PY{n}{rome} \PY{o}{=} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{rome}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cosine\PYZus{}similarity(father, mother) = }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{father}\PY{p}{,} \PY{n}{mother}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cosine\PYZus{}similarity(ball, crocodile) = }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{ball}\PY{p}{,} \PY{n}{crocodile}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cosine\PYZus{}similarity(france \PYZhy{} paris, rome \PYZhy{} italy) = }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{france} \PY{o}{\PYZhy{}} \PY{n}{paris}\PY{p}{,} \PY{n}{rome} \PY{o}{\PYZhy{}} \PY{n}{italy}\PY{p}{)}\PY{p}{)}
\PY{c+c1}{\PYZsh{} END SKIP FOR GRADING}

\PY{c+c1}{\PYZsh{} PUBLIC TESTS}
\PY{k}{def} \PY{n+nf}{cosine\PYZus{}similarity\PYZus{}test}\PY{p}{(}\PY{n}{target}\PY{p}{)}\PY{p}{:}
    \PY{n}{a} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{uniform}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}
    \PY{n}{b} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{uniform}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}
    \PY{n}{c} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{uniform}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{23}\PY{p}{)}
        
    \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{isclose}\PY{p}{(}\PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{a}\PY{p}{,} \PY{n}{a}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cosine\PYZus{}similarity(a, a) must be 1}\PY{l+s+s2}{\PYZdq{}}
    \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{isclose}\PY{p}{(}\PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{p}{(}\PY{n}{c} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{l+m+mi}{0}\PY{p}{)} \PY{o}{*} \PY{l+m+mi}{1}\PY{p}{,} \PY{p}{(}\PY{n}{c} \PY{o}{\PYZlt{}} \PY{l+m+mi}{0}\PY{p}{)} \PY{o}{*} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cosine\PYZus{}similarity(a, not(a)) must be 0}\PY{l+s+s2}{\PYZdq{}}
    \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{isclose}\PY{p}{(}\PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{a}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{n}{a}\PY{p}{)}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cosine\PYZus{}similarity(a, \PYZhy{}a) must be \PYZhy{}1}\PY{l+s+s2}{\PYZdq{}}
    \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{isclose}\PY{p}{(}\PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{a}\PY{p}{,} \PY{n}{b}\PY{p}{)}\PY{p}{,} \PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{a} \PY{o}{*} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{b} \PY{o}{*} \PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cosine\PYZus{}similarity must be scale\PYZhy{}independent. You must divide by the product of the norms of each input}\PY{l+s+s2}{\PYZdq{}}

    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}033}\PY{l+s+s2}{[92mAll test passed!}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    
\PY{n}{cosine\PYZus{}similarity\PYZus{}test}\PY{p}{(}\PY{n}{cosine\PYZus{}similarity}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
cosine\_similarity(father, mother) =  0.8909038442893615
cosine\_similarity(ball, crocodile) =  0.2743924626137942
cosine\_similarity(france - paris, rome - italy) =  -0.6751479308174201
\textcolor{ansi-green-intense}{All test passed!}
    \end{Verbatim}

    \hypertarget{try-different-words}{%
\paragraph{Try different words!}\label{try-different-words}}

After you get the correct expected output, please feel free to modify
the inputs and measure the cosine similarity between other pairs of
words! Playing around with the cosine similarity of other inputs will
give you a better sense of how word vectors behave.

    \#\# 4 - Word Analogy Task

\begin{itemize}
\item
  In the word analogy task, complete this sentence:\\
  ``\emph{a} is to \emph{b} as \emph{c} is to \textbf{\_\_\_\_}''.
\item
  An example is:\\
  `\emph{man} is to \emph{woman} as \emph{king} is to \emph{queen}' .
\item
  You're trying to find a word \emph{d}, such that the associated word
  vectors \(e_a, e_b, e_c, e_d\) are related in the following manner:\\
  \(e_b - e_a \approx e_d - e_c\)
\item
  Measure the similarity between \(e_b - e_a\) and \(e_d - e_c\) using
  cosine similarity.
\end{itemize}

\#\#\# Exercise 2 - complete\_analogy

Complete the code below to perform word analogies!

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} UNQ\PYZus{}C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)}
\PY{c+c1}{\PYZsh{} GRADED FUNCTION: complete\PYZus{}analogy}

\PY{k}{def} \PY{n+nf}{complete\PYZus{}analogy}\PY{p}{(}\PY{n}{word\PYZus{}a}\PY{p}{,} \PY{n}{word\PYZus{}b}\PY{p}{,} \PY{n}{word\PYZus{}c}\PY{p}{,} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Performs the word analogy task as explained above: a is to b as c is to \PYZus{}\PYZus{}\PYZus{}\PYZus{}. }
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Arguments:}
\PY{l+s+sd}{    word\PYZus{}a \PYZhy{}\PYZhy{} a word, string}
\PY{l+s+sd}{    word\PYZus{}b \PYZhy{}\PYZhy{} a word, string}
\PY{l+s+sd}{    word\PYZus{}c \PYZhy{}\PYZhy{} a word, string}
\PY{l+s+sd}{    word\PYZus{}to\PYZus{}vec\PYZus{}map \PYZhy{}\PYZhy{} dictionary that maps words to their corresponding vectors. }
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{    best\PYZus{}word \PYZhy{}\PYZhy{}  the word such that v\PYZus{}b \PYZhy{} v\PYZus{}a is close to v\PYZus{}best\PYZus{}word \PYZhy{} v\PYZus{}c, as measured by cosine similarity}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{c+c1}{\PYZsh{} convert words to lowercase}
    \PY{n}{word\PYZus{}a}\PY{p}{,} \PY{n}{word\PYZus{}b}\PY{p}{,} \PY{n}{word\PYZus{}c} \PY{o}{=} \PY{n}{word\PYZus{}a}\PY{o}{.}\PY{n}{lower}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{word\PYZus{}b}\PY{o}{.}\PY{n}{lower}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{word\PYZus{}c}\PY{o}{.}\PY{n}{lower}\PY{p}{(}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
    \PY{c+c1}{\PYZsh{} Get the word embeddings e\PYZus{}a, e\PYZus{}b and e\PYZus{}c (≈1\PYZhy{}3 lines)}
    \PY{n}{e\PYZus{}a}\PY{p}{,} \PY{n}{e\PYZus{}b}\PY{p}{,} \PY{n}{e\PYZus{}c} \PY{o}{=} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{n}{word\PYZus{}a}\PY{p}{]}\PY{p}{,} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{n}{word\PYZus{}b}\PY{p}{]}\PY{p}{,} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{n}{word\PYZus{}c}\PY{p}{]}
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
    
    \PY{n}{words} \PY{o}{=} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{o}{.}\PY{n}{keys}\PY{p}{(}\PY{p}{)}
    \PY{n}{max\PYZus{}cosine\PYZus{}sim} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{100}              \PY{c+c1}{\PYZsh{} Initialize max\PYZus{}cosine\PYZus{}sim to a large negative number}
    \PY{n}{best\PYZus{}word} \PY{o}{=} \PY{k+kc}{None}                   \PY{c+c1}{\PYZsh{} Initialize best\PYZus{}word with None, it will help keep track of the word to output}
    
    \PY{c+c1}{\PYZsh{} loop over the whole word vector set}
    \PY{k}{for} \PY{n}{w} \PY{o+ow}{in} \PY{n}{words}\PY{p}{:}   
        \PY{c+c1}{\PYZsh{} to avoid best\PYZus{}word being one the input words, skip the input word\PYZus{}c}
        \PY{c+c1}{\PYZsh{} skip word\PYZus{}c from query}
        \PY{k}{if} \PY{n}{w} \PY{o}{==} \PY{n}{word\PYZus{}c}\PY{p}{:}
            \PY{k}{continue}
        
        \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
        \PY{c+c1}{\PYZsh{} Compute cosine similarity between the vector (e\PYZus{}b \PYZhy{} e\PYZus{}a) and the vector ((w\PYZsq{}s vector representation) \PYZhy{} e\PYZus{}c)  (≈1 line)}
        \PY{n}{cosine\PYZus{}sim} \PY{o}{=} \PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{e\PYZus{}b} \PY{o}{\PYZhy{}} \PY{n}{e\PYZus{}a}\PY{p}{,} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{n}{w}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{e\PYZus{}c}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} If the cosine\PYZus{}sim is more than the max\PYZus{}cosine\PYZus{}sim seen so far,}
            \PY{c+c1}{\PYZsh{} then: set the new max\PYZus{}cosine\PYZus{}sim to the current cosine\PYZus{}sim and the best\PYZus{}word to the current word (≈3 lines)}
        \PY{k}{if} \PY{n}{cosine\PYZus{}sim} \PY{o}{\PYZgt{}} \PY{n}{max\PYZus{}cosine\PYZus{}sim}\PY{p}{:}
            \PY{n}{max\PYZus{}cosine\PYZus{}sim} \PY{o}{=} \PY{n}{cosine\PYZus{}sim}
            \PY{n}{best\PYZus{}word} \PY{o}{=} \PY{n}{w}
        \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
        
    \PY{k}{return} \PY{n}{best\PYZus{}word}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} PUBLIC TEST}
\PY{k}{def} \PY{n+nf}{complete\PYZus{}analogy\PYZus{}test}\PY{p}{(}\PY{n}{target}\PY{p}{)}\PY{p}{:}
    \PY{n}{a} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{]} \PY{c+c1}{\PYZsh{} Center at a}
    \PY{n}{a\PYZus{}nw} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{]} \PY{c+c1}{\PYZsh{} North\PYZhy{}West oriented vector from a}
    \PY{n}{a\PYZus{}s} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]} \PY{c+c1}{\PYZsh{} South oriented vector from a}
    
    \PY{n}{c} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]} \PY{c+c1}{\PYZsh{} Center at c}
    \PY{c+c1}{\PYZsh{} Create a controlled word to vec map}
    \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{a}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{a}\PY{p}{,}
                       \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{synonym\PYZus{}of\PYZus{}a}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{a}\PY{p}{,}
                       \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{a\PYZus{}nw}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{a\PYZus{}nw}\PY{p}{,} 
                       \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{a\PYZus{}s}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{a\PYZus{}s}\PY{p}{,} 
                       \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{c}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{c}\PY{p}{,} 
                       \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{c\PYZus{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{c+c1}{\PYZsh{} N}
                       \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{c\PYZus{}ne}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{c+c1}{\PYZsh{} NE}
                       \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{c\PYZus{}e}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{c+c1}{\PYZsh{} E}
                       \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{c\PYZus{}se}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{c+c1}{\PYZsh{} SE}
                       \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{c\PYZus{}s}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{c+c1}{\PYZsh{} S}
                       \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{c\PYZus{}sw}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{c+c1}{\PYZsh{} SW}
                       \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{c\PYZus{}w}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{c+c1}{\PYZsh{} W}
                       \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{c\PYZus{}nw}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]} \PY{c+c1}{\PYZsh{} NW}
                      \PY{p}{\PYZcb{}}
    
    \PY{c+c1}{\PYZsh{} Convert lists to np.arrays}
    \PY{k}{for} \PY{n}{key} \PY{o+ow}{in} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{o}{.}\PY{n}{keys}\PY{p}{(}\PY{p}{)}\PY{p}{:}
        \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{n}{key}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{n}{key}\PY{p}{]}\PY{p}{)}
            
    \PY{k}{assert}\PY{p}{(}\PY{n}{target}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{a}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{a\PYZus{}nw}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{c}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{)} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{c\PYZus{}nw}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{k}{assert}\PY{p}{(}\PY{n}{target}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{a}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{a\PYZus{}s}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{c}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{)} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{c\PYZus{}s}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{k}{assert}\PY{p}{(}\PY{n}{target}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{a}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{synonym\PYZus{}of\PYZus{}a}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{c}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{)} \PY{o}{!=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{c}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Best word cannot be input query}\PY{l+s+s2}{\PYZdq{}}
    \PY{k}{assert}\PY{p}{(}\PY{n}{target}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{a}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{c}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{a}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{)} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{c}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}033}\PY{l+s+s2}{[92mAll tests passed}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    
\PY{n}{complete\PYZus{}analogy\PYZus{}test}\PY{p}{(}\PY{n}{complete\PYZus{}analogy}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textcolor{ansi-green-intense}{All tests passed}
    \end{Verbatim}

    Run the cell below to test your code. Patience, young
grasshopper\ldots this may take 1-2 minutes.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} START SKIP FOR GRADING}
\PY{n}{triads\PYZus{}to\PYZus{}try} \PY{o}{=} \PY{p}{[}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{italy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{italian}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{spain}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{india}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{delhi}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{japan}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{man}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{woman}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{boy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{small}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{smaller}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{large}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{]}
\PY{k}{for} \PY{n}{triad} \PY{o+ow}{in} \PY{n}{triads\PYZus{}to\PYZus{}try}\PY{p}{:}
    \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ \PYZhy{}\PYZgt{} }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ :: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ \PYZhy{}\PYZgt{} }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(} \PY{o}{*}\PY{n}{triad}\PY{p}{,} \PY{n}{complete\PYZus{}analogy}\PY{p}{(}\PY{o}{*}\PY{n}{triad}\PY{p}{,} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{)}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} END SKIP FOR GRADING}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
italy -> italian :: spain -> spanish
india -> delhi :: japan -> tokyo
man -> woman :: boy -> girl
small -> smaller :: large -> smaller
    \end{Verbatim}

    Once you get the output, try modifying the input cells above to test
your own analogies.

\textbf{Hint}: Try to find some other analogy pairs that will work,
along with some others where the algorithm doesn't give the right
answer: * For example, you can try small-\textgreater smaller as
big-\textgreater?

    \hypertarget{congratulations}{%
\subsection{Congratulations!}\label{congratulations}}

You've come to the end of the graded portion of the assignment. By now,
you've:

\begin{itemize}
\tightlist
\item
  Loaded some pre-trained word vectors
\item
  Measured the similarity between word vectors using cosine similarity
\item
  Used word embeddings to solve word analogy problems such as Man is to
  Woman as King is to \_\_.
\end{itemize}

Cosine similarity is a relatively simple and intuitive, yet powerful,
method you can use to capture nuanced relationships between words. These
exercises should be helpful to you in explaining how it works, and
applying it to your own projects!

    What you should remember:

\begin{itemize}
\tightlist
\item
  Cosine similarity is a good way to compare the similarity between
  pairs of word vectors.

  \begin{itemize}
  \tightlist
  \item
    Note that L2 (Euclidean) distance also works.
  \end{itemize}
\item
  For NLP applications, using a pre-trained set of word vectors is often
  a great way to get started.
\end{itemize}

Even though you've finished the graded portion, please take a look at
the rest of this notebook to learn about debiasing word vectors.

    \#\# 5 - Debiasing Word Vectors (OPTIONAL/UNGRADED)

    In the following exercise, you'll examine gender biases that can be
reflected in a word embedding, and explore algorithms for reducing the
bias. In addition to learning about the topic of debiasing, this
exercise will also help hone your intuition about what word vectors are
doing. This section involves a bit of linear algebra, though you can
certainly complete it without being an expert! Go ahead and give it a
shot. This portion of the notebook is optional and is not
graded\ldots so just have fun and explore.

First, see how the GloVe word embeddings relate to gender. You'll begin
by computing a vector \(g = e_{woman}-e_{man}\), where \(e_{woman}\)
represents the word vector corresponding to the word \emph{woman}, and
\(e_{man}\) corresponds to the word vector corresponding to the word
\emph{man}. The resulting vector \(g\) roughly encodes the concept of
``gender''.

You might get a more accurate representation if you compute
\(g_1 = e_{mother}-e_{father}\), \(g_2 = e_{girl}-e_{boy}\), etc. and
average over them, but just using \(e_{woman}-e_{man}\) will give good
enough results for now.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{g} \PY{o}{=} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{woman}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{man}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{g}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[-0.087144    0.2182     -0.40986    -0.03922    -0.1032      0.94165
 -0.06042     0.32988     0.46144    -0.35962     0.31102    -0.86824
  0.96006     0.01073     0.24337     0.08193    -1.02722    -0.21122
  0.695044   -0.00222     0.29106     0.5053     -0.099454    0.40445
  0.30181     0.1355     -0.0606     -0.07131    -0.19245    -0.06115
 -0.3204      0.07165    -0.13337    -0.25068714 -0.14293    -0.224957
 -0.149       0.048882    0.12191    -0.27362    -0.165476   -0.20426
  0.54376    -0.271425   -0.10245    -0.32108     0.2516     -0.33455
 -0.04371     0.01258   ]
    \end{Verbatim}

    Now, consider the cosine similarity of different words with \(g\). What
does a positive value of similarity mean, versus a negative cosine
similarity?

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{List of names and their similarities with constructed vector:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} girls and boys name}
\PY{n}{name\PYZus{}list} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{john}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{marie}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sophie}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ronaldo}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{priya}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rahul}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{danielle}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{reza}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{katy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yasmin}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}

\PY{k}{for} \PY{n}{w} \PY{o+ow}{in} \PY{n}{name\PYZus{}list}\PY{p}{:}
    \PY{n+nb}{print} \PY{p}{(}\PY{n}{w}\PY{p}{,} \PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{n}{w}\PY{p}{]}\PY{p}{,} \PY{n}{g}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
List of names and their similarities with constructed vector:
john -0.23163356145973724
marie 0.315597935396073
sophie 0.3186878985941878
ronaldo -0.31244796850329437
priya 0.17632041839009402
rahul -0.16915471039231722
danielle 0.24393299216283895
reza -0.07930429672199553
katy 0.2831068659572615
yasmin 0.23313857767928753
    \end{Verbatim}

    As you can see, female first names tend to have a positive cosine
similarity with our constructed vector \(g\), while male first names
tend to have a negative cosine similarity. This is not surprising, and
the result seems acceptable.

Now try with some other words:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Other words and their similarities:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{word\PYZus{}list} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lipstick}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{guns}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{science}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{arts}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{literature}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{warrior}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{doctor}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tree}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{receptionist}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{technology}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}  \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{fashion}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{teacher}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{engineer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pilot}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{computer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{singer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\PY{k}{for} \PY{n}{w} \PY{o+ow}{in} \PY{n}{word\PYZus{}list}\PY{p}{:}
    \PY{n+nb}{print} \PY{p}{(}\PY{n}{w}\PY{p}{,} \PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{n}{w}\PY{p}{]}\PY{p}{,} \PY{n}{g}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Other words and their similarities:
lipstick 0.27691916256382665
guns -0.1888485567898898
science -0.06082906540929699
arts 0.008189312385880344
literature 0.0647250443345993
warrior -0.20920164641125288
doctor 0.11895289410935045
tree -0.07089399175478092
receptionist 0.3307794175059374
technology -0.13193732447554293
fashion 0.035638946257727
teacher 0.1792092343182567
engineer -0.08039280494524072
pilot 0.0010764498991917074
computer -0.10330358873850498
singer 0.18500518136496297
    \end{Verbatim}

    Do you notice anything surprising? It is astonishing how these results
reflect certain unhealthy gender stereotypes. For example, we see
``computer'' is negative and is closer in value to male first names,
while ``literature'' is positive and is closer to female first names.
Ouch!

You'll see below how to reduce the bias of these vectors, using an
algorithm due to \href{https://arxiv.org/abs/1607.06520}{Boliukbasi et
al., 2016}. Note that some word pairs such as ``actor''/``actress'' or
``grandmother''/``grandfather'' should remain gender-specific, while
other words such as ``receptionist'' or ``technology'' should be
neutralized, i.e.~not be gender-related. You'll have to treat these two
types of words differently when debiasing.

\#\#\# 5.1 - Neutralize Bias for Non-Gender Specific Words

The figure below should help you visualize what neutralizing does. If
you're using a 50-dimensional word embedding, the 50 dimensional space
can be split into two parts: The bias-direction \(g\), and the remaining
49 dimensions, which is called \(g_{\perp}\) here. In linear algebra, we
say that the 49-dimensional \(g_{\perp}\) is perpendicular (or
``orthogonal'') to \(g\), meaning it is at 90 degrees to \(g\). The
neutralization step takes a vector such as \(e_{receptionist}\) and
zeros out the component in the direction of \(g\), giving us
\(e_{receptionist}^{debiased}\).

Even though \(g_{\perp}\) is 49-dimensional, given the limitations of
what you can draw on a 2D screen, it's illustrated using a 1-dimensional
axis below.

Figure 2: The word vector for ``receptionist'' represented before and
after applying the neutralize operation.

\#\#\# Exercise 3 - neutralize

Implement \texttt{neutralize()} to remove the bias of words such as
``receptionist'' or ``scientist.''

Given an input embedding \(e\), you can use the following formulas to
compute \(e^{debiased}\):

\[e^{bias\_component} = \frac{e \cdot g}{||g||_2^2} * g\tag{2}\]
\[e^{debiased} = e - e^{bias\_component}\tag{3}\]

If you are an expert in linear algebra, you may recognize
\(e^{bias\_component}\) as the projection of \(e\) onto the direction
\(g\). If you're not an expert in linear algebra, don't worry about
this. ;)

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{neutralize}\PY{p}{(}\PY{n}{word}\PY{p}{,} \PY{n}{g}\PY{p}{,} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Removes the bias of \PYZdq{}word\PYZdq{} by projecting it on the space orthogonal to the bias axis. }
\PY{l+s+sd}{    This function ensures that gender neutral words are zero in the gender subspace.}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Arguments:}
\PY{l+s+sd}{        word \PYZhy{}\PYZhy{} string indicating the word to debias}
\PY{l+s+sd}{        g \PYZhy{}\PYZhy{} numpy\PYZhy{}array of shape (50,), corresponding to the bias axis (such as gender)}
\PY{l+s+sd}{        word\PYZus{}to\PYZus{}vec\PYZus{}map \PYZhy{}\PYZhy{} dictionary mapping words to their corresponding vectors.}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{        e\PYZus{}debiased \PYZhy{}\PYZhy{} neutralized word vector representation of the input \PYZdq{}word\PYZdq{}}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
    \PY{c+c1}{\PYZsh{} Select word vector representation of \PYZdq{}word\PYZdq{}. Use word\PYZus{}to\PYZus{}vec\PYZus{}map. (≈ 1 line)}
    \PY{n}{e} \PY{o}{=} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{n}{word}\PY{p}{]}
    
    \PY{c+c1}{\PYZsh{} Compute e\PYZus{}biascomponent using the formula given above. (≈ 1 line)}
    \PY{n}{e\PYZus{}biascomponent} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{e} \PY{p}{,}\PY{n}{g}\PY{p}{)} \PY{o}{/} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{g} \PY{o}{*} \PY{n}{g}\PY{p}{)} \PY{o}{*} \PY{n}{g}
 
    \PY{c+c1}{\PYZsh{} Neutralize e by subtracting e\PYZus{}biascomponent from it }
    \PY{c+c1}{\PYZsh{} e\PYZus{}debiased should be equal to its orthogonal projection. (≈ 1 line)}
    \PY{n}{e\PYZus{}debiased} \PY{o}{=} \PY{n}{e} \PY{o}{\PYZhy{}} \PY{n}{e\PYZus{}biascomponent}
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
    
    \PY{k}{return} \PY{n}{e\PYZus{}debiased}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{e} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{receptionist}\PY{l+s+s2}{\PYZdq{}}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cosine similarity between }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n}{e} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ and g, before neutralizing: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{receptionist}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,} \PY{n}{g}\PY{p}{)}\PY{p}{)}

\PY{n}{e\PYZus{}debiased} \PY{o}{=} \PY{n}{neutralize}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{receptionist}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{g}\PY{p}{,} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cosine similarity between }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n}{e} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ and g, after neutralizing: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{e\PYZus{}debiased}\PY{p}{,} \PY{n}{g}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
cosine similarity between receptionist and g, before neutralizing:
0.3307794175059374
cosine similarity between receptionist and g, after neutralizing:
-4.442232511624783e-17
    \end{Verbatim}

    \textbf{Expected Output}: The second result is essentially 0, up to
numerical rounding (on the order of \(10^{-17}\)).

cosine similarity between receptionist and g, before neutralizing: :

0.330779417506

\begin{verbatim}
<tr>
    <td>
        <b>cosine similarity between receptionist and g, after neutralizing</b> :
    </td>
    <td>
     -4.442232511624783e-17
</tr>
\end{verbatim}

    \#\#\# 5.2 - Equalization Algorithm for Gender-Specific Words

Next, let's see how debiasing can also be applied to word pairs such as
``actress'' and ``actor.'' Equalization is applied to pairs of words
that you might want to have differ only through the gender property. As
a concrete example, suppose that ``actress'' is closer to ``babysit''
than ``actor.'' By applying neutralization to ``babysit,'' you can
reduce the gender stereotype associated with babysitting. But this still
does not guarantee that ``actor'' and ``actress'' are equidistant from
``babysit.'' The equalization algorithm takes care of this.

The key idea behind equalization is to make sure that a particular pair
of words are equidistant from the 49-dimensional \(g_\perp\). The
equalization step also ensures that the two equalized steps are now the
same distance from \(e_{receptionist}^{debiased}\), or from any other
work that has been neutralized. Visually, this is how equalization
works:

The derivation of the linear algebra to do this is a bit more complex.
(See Bolukbasi et al., 2016 in the References for details.) Here are the
key equations:

\[ \mu = \frac{e_{w1} + e_{w2}}{2}\tag{4}\]

\[ \mu_{B} = \frac {\mu \cdot \text{bias_axis}}{||\text{bias_axis}||_2^2} *\text{bias_axis}
\tag{5}\]

\[\mu_{\perp} = \mu - \mu_{B} \tag{6}\]

\[ e_{w1B} = \frac {e_{w1} \cdot \text{bias_axis}}{||\text{bias_axis}||_2^2} *\text{bias_axis}
\tag{7}\]
\[ e_{w2B} = \frac {e_{w2} \cdot \text{bias_axis}}{||\text{bias_axis}||_2^2} *\text{bias_axis}
\tag{8}\]

\[e_{w1B}^{corrected} = \sqrt{ |{1 - ||\mu_{\perp} ||^2_2} |} * \frac{e_{\text{w1B}} - \mu_B} {||(e_{w1} - \mu_{\perp}) - \mu_B||_2} \tag{9}\]

\[e_{w2B}^{corrected} = \sqrt{ |{1 - ||\mu_{\perp} ||^2_2} |} * \frac{e_{\text{w2B}} - \mu_B} {||(e_{w2} - \mu_{\perp}) - \mu_B||_2} \tag{10}\]

\[e_1 = e_{w1B}^{corrected} + \mu_{\perp} \tag{11}\]
\[e_2 = e_{w2B}^{corrected} + \mu_{\perp} \tag{12}\]

\#\#\# Exercise 4 - equalize

Implement the \texttt{equalize()} function below.

Use the equations above to get the final equalized version of the pair
of words. Good luck!

\textbf{Hint} - Use
\href{https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html}{np.linalg.norm}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{equalize}\PY{p}{(}\PY{n}{pair}\PY{p}{,} \PY{n}{bias\PYZus{}axis}\PY{p}{,} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Debias gender specific words by following the equalize method described in the figure above.}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Arguments:}
\PY{l+s+sd}{    pair \PYZhy{}\PYZhy{} pair of strings of gender specific words to debias, e.g. (\PYZdq{}actress\PYZdq{}, \PYZdq{}actor\PYZdq{}) }
\PY{l+s+sd}{    bias\PYZus{}axis \PYZhy{}\PYZhy{} numpy\PYZhy{}array of shape (50,), vector corresponding to the bias axis, e.g. gender}
\PY{l+s+sd}{    word\PYZus{}to\PYZus{}vec\PYZus{}map \PYZhy{}\PYZhy{} dictionary mapping words to their corresponding vectors}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns}
\PY{l+s+sd}{    e\PYZus{}1 \PYZhy{}\PYZhy{} word vector corresponding to the first word}
\PY{l+s+sd}{    e\PYZus{}2 \PYZhy{}\PYZhy{} word vector corresponding to the second word}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
    \PY{c+c1}{\PYZsh{} Step 1: Select word vector representation of \PYZdq{}word\PYZdq{}. Use word\PYZus{}to\PYZus{}vec\PYZus{}map. (≈ 2 lines)}
    \PY{n}{w1}\PY{p}{,} \PY{n}{w2} \PY{o}{=} \PY{n}{pair}
    \PY{n}{e\PYZus{}w1}\PY{p}{,} \PY{n}{e\PYZus{}w2} \PY{o}{=} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{n}{w1}\PY{p}{]}\PY{p}{,}\PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{n}{w2}\PY{p}{]}
    
    \PY{c+c1}{\PYZsh{} Step 2: Compute the mean of e\PYZus{}w1 and e\PYZus{}w2 (≈ 1 line)}
    \PY{n}{mu} \PY{o}{=} \PY{p}{(}\PY{n}{e\PYZus{}w1} \PY{o}{+} \PY{n}{e\PYZus{}w2}\PY{p}{)} \PY{o}{/} \PY{l+m+mi}{2}

    \PY{c+c1}{\PYZsh{} Step 3: Compute the projections of mu over the bias axis and the orthogonal axis (≈ 2 lines)}
    \PY{n}{mu\PYZus{}B} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{mu}\PY{p}{,} \PY{n}{bias\PYZus{}axis}\PY{p}{)} \PY{o}{/} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{bias\PYZus{}axis} \PY{o}{*} \PY{n}{bias\PYZus{}axis}\PY{p}{)} \PY{o}{*} \PY{n}{bias\PYZus{}axis}
    \PY{n}{mu\PYZus{}orth} \PY{o}{=} \PY{n}{mu} \PY{o}{\PYZhy{}} \PY{n}{mu\PYZus{}B}

    \PY{c+c1}{\PYZsh{} Step 4: Use equations (7) and (8) to compute e\PYZus{}w1B and e\PYZus{}w2B (≈2 lines)}
    \PY{n}{e\PYZus{}w1B} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{e\PYZus{}w1}\PY{p}{,} \PY{n}{bias\PYZus{}axis}\PY{p}{)} \PY{o}{/} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{bias\PYZus{}axis} \PY{o}{*} \PY{n}{bias\PYZus{}axis}\PY{p}{)} \PY{o}{*} \PY{n}{bias\PYZus{}axis}
    \PY{n}{e\PYZus{}w2B} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{e\PYZus{}w2}\PY{p}{,} \PY{n}{bias\PYZus{}axis}\PY{p}{)} \PY{o}{/} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{bias\PYZus{}axis} \PY{o}{*} \PY{n}{bias\PYZus{}axis}\PY{p}{)} \PY{o}{*} \PY{n}{bias\PYZus{}axis}
        
    \PY{c+c1}{\PYZsh{} Step 5: Adjust the Bias part of e\PYZus{}w1B and e\PYZus{}w2B using the formulas (9) and (10) given above (≈2 lines)}
    \PY{n}{corrected\PYZus{}e\PYZus{}w1B} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{mu\PYZus{}orth} \PY{o}{*} \PY{n}{mu\PYZus{}orth}\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{o}{*} \PY{p}{(}\PY{n}{e\PYZus{}w1B} \PY{o}{\PYZhy{}} \PY{n}{mu\PYZus{}B}\PY{p}{)} \PY{o}{/} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{e\PYZus{}w1} \PY{o}{\PYZhy{}} \PY{n}{mu\PYZus{}orth} \PY{o}{\PYZhy{}} \PY{n}{mu\PYZus{}B}\PY{p}{)}
    \PY{n}{corrected\PYZus{}e\PYZus{}w2B} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{mu\PYZus{}orth} \PY{o}{*} \PY{n}{mu\PYZus{}orth}\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{o}{*} \PY{p}{(}\PY{n}{e\PYZus{}w2B} \PY{o}{\PYZhy{}} \PY{n}{mu\PYZus{}B}\PY{p}{)} \PY{o}{/} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{e\PYZus{}w2} \PY{o}{\PYZhy{}} \PY{n}{mu\PYZus{}orth} \PY{o}{\PYZhy{}} \PY{n}{mu\PYZus{}B}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Step 6: Debias by equalizing e1 and e2 to the sum of their corrected projections (≈2 lines)}
    \PY{n}{e1} \PY{o}{=} \PY{n}{corrected\PYZus{}e\PYZus{}w1B} \PY{o}{+} \PY{n}{mu\PYZus{}orth}
    \PY{n}{e2} \PY{o}{=} \PY{n}{corrected\PYZus{}e\PYZus{}w2B} \PY{o}{+} \PY{n}{mu\PYZus{}orth}
                                                                
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
    
    \PY{k}{return} \PY{n}{e1}\PY{p}{,} \PY{n}{e2}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cosine similarities before equalizing:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cosine\PYZus{}similarity(word\PYZus{}to\PYZus{}vec\PYZus{}map[}\PY{l+s+se}{\PYZbs{}\PYZdq{}}\PY{l+s+s2}{man}\PY{l+s+se}{\PYZbs{}\PYZdq{}}\PY{l+s+s2}{], gender) = }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{man}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,} \PY{n}{g}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cosine\PYZus{}similarity(word\PYZus{}to\PYZus{}vec\PYZus{}map[}\PY{l+s+se}{\PYZbs{}\PYZdq{}}\PY{l+s+s2}{woman}\PY{l+s+se}{\PYZbs{}\PYZdq{}}\PY{l+s+s2}{], gender) = }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{woman}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,} \PY{n}{g}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
\PY{n}{e1}\PY{p}{,} \PY{n}{e2} \PY{o}{=} \PY{n}{equalize}\PY{p}{(}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{man}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{woman}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{,} \PY{n}{g}\PY{p}{,} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cosine similarities after equalizing:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cosine\PYZus{}similarity(e1, gender) = }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{e1}\PY{p}{,} \PY{n}{g}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cosine\PYZus{}similarity(e2, gender) = }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{e2}\PY{p}{,} \PY{n}{g}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
cosine similarities before equalizing:
cosine\_similarity(word\_to\_vec\_map["man"], gender) =  -0.11711095765336832
cosine\_similarity(word\_to\_vec\_map["woman"], gender) =  0.35666618846270376

cosine similarities after equalizing:
cosine\_similarity(e1, gender) =  -0.7004364289309388
cosine\_similarity(e2, gender) =  0.7004364289309388
    \end{Verbatim}

    \textbf{Expected Output}:

cosine similarities before equalizing:

cosine\_similarity(word\_to\_vec\_map{[}``man''{]}, gender) =

-0.117110957653

\begin{verbatim}
<tr>
    <td>
        <b>cosine_similarity(word_to_vec_map["woman"], gender)</b> =
    </td>
    <td>
     0.356666188463
    </td>
</tr>
\end{verbatim}

cosine similarities after equalizing:

cosine\_similarity(e1, gender) =

-0.942653373599985

\begin{verbatim}
<tr>
    <td>
        <b>cosine_similarity(e2, gender)</b> =
    </td>
    <td>
     0.9231551731025899
    </td>
</tr>
\end{verbatim}

    Go ahead and play with the input words in the cell above, to apply
equalization to other pairs of words.

Hint: Try\ldots{}

These debiasing algorithms are very helpful for reducing bias, but
aren't perfect and don't eliminate all traces of bias. For example, one
weakness of this implementation was that the bias direction \(g\) was
defined using only the pair of words \emph{woman} and \emph{man}. As
discussed earlier, if \(g\) were defined by computing
\(g_1 = e_{woman} - e_{man}\); \(g_2 = e_{mother} - e_{father}\);
\(g_3 = e_{girl} - e_{boy}\); and so on and averaging over them, you
would obtain a better estimate of the ``gender'' dimension in the 50
dimensional word embedding space. Feel free to play with these types of
variants as well!

    \hypertarget{congratulations}{%
\subsubsection{Congratulations!}\label{congratulations}}

You have come to the end of both graded and ungraded portions of this
notebook, and have seen several of the ways that word vectors can be
applied and modified. Great work pushing your knowledge in the areas of
neutralizing and equalizing word vectors! See you next time.

    \#\# 6 - References

\begin{itemize}
\tightlist
\item
  The debiasing algorithm is from Bolukbasi et al., 2016,
  \href{https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf}{Man
  is to Computer Programmer as Woman is to Homemaker? Debiasing Word
  Embeddings}
\item
  The GloVe word embeddings were due to Jeffrey Pennington, Richard
  Socher, and Christopher D. Manning.
  (https://nlp.stanford.edu/projects/glove/)
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]

\end{Verbatim}
\end{tcolorbox}


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
